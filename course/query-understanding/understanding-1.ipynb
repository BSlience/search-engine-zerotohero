{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923c9d3b-0671-43ba-92d0-b36400b46d41",
   "metadata": {},
   "source": [
    "# Query 理解（一）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efec14a-35bb-4ebc-8cb2-dec1a46ab6b9",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e1f8b5-0675-4040-807c-0e7849f5b236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencc\n",
      "  Downloading OpenCC-1.1.3-cp36-cp36m-manylinux1_x86_64.whl (766 kB)\n",
      "     |████████████████████████████████| 766 kB 996 kB/s            \n",
      "\u001b[?25hInstalling collected packages: opencc\n",
      "Successfully installed opencc-1.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204dcb2d-08bb-4d4a-8836-10f902c7b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import opencc\n",
    "\n",
    "\n",
    "class QueryPre:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, ustring):\n",
    "        ustring = self.filter_emoji(ustring)\n",
    "        ustring = self.strQ2B(ustring)\n",
    "        ustring = self.t2s_by_opencc(ustring)\n",
    "        ustring = self.capital_to_lower(ustring)\n",
    "        return ustring\n",
    "\n",
    "    def strQ2B(self, ustring):\n",
    "        \"\"\"把字符串全角转半角\"\"\"\n",
    "        rstring = \"\"\n",
    "        for uchar in ustring:\n",
    "            # ord返回对应的ascii数值，\n",
    "            inside_code = ord(uchar)\n",
    "            # 全角空格直接转换\n",
    "            if inside_code == 12288:\n",
    "                inside_code = 32\n",
    "            # 全角字符（除空格）根据关系转化\n",
    "            elif inside_code >= 65281 and inside_code <= 65374:\n",
    "                inside_code -= 65248\n",
    "            rstring += chr(inside_code)\n",
    "        return rstring\n",
    "\n",
    "    def strB2Q(self, ustring):\n",
    "        \"\"\"把字符串半角转全角\"\"\"\n",
    "        rstring = \"\"\n",
    "        for uchar in ustring:\n",
    "            # ord返回对应的ascii数值，\n",
    "            inside_code = ord(uchar)\n",
    "            # 半角空格直接转换\n",
    "            if inside_code == 32:\n",
    "                inside_code = 12288\n",
    "            elif inside_code >= 33 and inside_code <= 126:\n",
    "                inside_code += 65248\n",
    "                # chr输入一个整数返回其对应的ascii字符\n",
    "            rstring += chr(inside_code)\n",
    "        return rstring\n",
    "\n",
    "    def capital_to_lower(self, ustring):\n",
    "        \"\"\"\n",
    "         大写转小写\n",
    "        :param ustring: 字符串\n",
    "        :return: 所有字母都小写后的字符串\n",
    "        \"\"\"\n",
    "        return ustring.lower()\n",
    "\n",
    "    def t2s_by_opencc(self, ustring):\n",
    "        \"\"\"\n",
    "        繁体转简体\n",
    "        :param ustring: 繁体string\n",
    "        :return: 简体string\n",
    "        \"\"\"\n",
    "        return opencc.OpenCC(\"t2s.json\").convert(ustring)\n",
    "\n",
    "    def filter_emoji(self, desstr, restr=\"\"):\n",
    "        \"\"\"\n",
    "        清除表情\n",
    "        :param desstr: 需要过滤的字符串\n",
    "        :param restr: 被替换成什么字符\n",
    "        :return: 返回处理结果\n",
    "        \"\"\"\n",
    "        # 过滤表情\n",
    "        try:\n",
    "            co = re.compile(\"[\\U00010000-\\U0010ffff]\")\n",
    "        except re.error:\n",
    "            co = re.compile(\"[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]\")\n",
    "        return co.sub(restr, desstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2010f4-fde3-4554-b831-09480caf372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "众所周知,长时间以来,加入欧盟和北约,就是乌克兰政府的心愿。过去这段时间里,为了加入北约和欧盟,乌克兰政府曾多次向其喊话,要求其同意乌克兰的加入。而自俄乌冲突正式开始后,乌克兰总统泽连斯基喊话欧盟与北约的频率更是越发频繁,就在前几日,泽连斯基更是发表了视频讲话,并在视频讲话中呼吁欧盟启动特殊程序,立即同意乌克兰加入欧盟。\n"
     ]
    }
   ],
   "source": [
    "p = QueryPre()\n",
    "print(p.run(\"众所周知，长时间以来，加入欧盟和北约，就是乌克兰政府的心愿。过去这段时间里，为了加入北约和欧盟，乌克兰政府曾多次向其喊话，要求其同意乌克兰的加入。而自俄乌冲突正式开始后，乌克兰总统泽连斯基喊话欧盟与北约的频率更是越发频繁，就在前几日，泽连斯基更是发表了视频讲话，并在视频讲话中呼吁欧盟启动特殊程序，立即同意乌克兰加入欧盟。\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038187e4-b818-4969-b125-beaa8c6b3073",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47e6593-d90b-4f15-9e0f-89cd1a522726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hanlp\n",
    "\n",
    "\n",
    "class Tokenization:\n",
    "    def __init__(self):\n",
    "        # 先加载模型\n",
    "        self.HanLP = hanlp.load(\n",
    "            hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH\n",
    "        )\n",
    "        \n",
    "    def stopwords(self, stopwords):\n",
    "        \n",
    "\n",
    "    def hanlp_token_ner(self, query):\n",
    "        \"\"\"\n",
    "        调用hanlp分词\n",
    "        :param query: 一句话\n",
    "        :return: 分词结果\n",
    "        \"\"\"\n",
    "        # 这里如果是空下面的分词会报错\n",
    "        if query.strip() == \"\":\n",
    "            return [\"\"]\n",
    "        # 精分&实体识别\n",
    "        results_document = self.HanLP(query, tasks=\"ner\")\n",
    "        return results_document[\"tok/fine\"], results_document[\"ner/msra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0836be8-14a9-4f08-9a16-72894ef87c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['众所周知', '，', '长', '时间', '以来', '，', '加入', '欧盟', '和', '北约', '，', '就', '是', '乌克兰', '政府', '的', '心愿', '。', '过去', '这', '段', '时间', '里', '，', '为了', '加入', '北约', '和', '欧盟', '，', '乌克兰', '政府', '曾', '多次', '向', '其', '喊话', '，', '要求', '其', '同意', '乌克兰', '的', '加入', '。', '而', '自', '俄', '乌', '冲突', '正式', '开始', '后', '，', '乌克兰', '总统', '泽连斯基', '喊话', '欧盟', '与', '北约', '的', '频率', '更是', '越发', '频繁', '，', '就', '在', '前', '几日', '，', '泽连斯基', '更', '是', '发表', '了', '视频', '讲话', '，', '并', '在', '视频', '讲话', '中', '呼吁', '欧盟', '启动', '特殊', '程序', '，', '立即', '同意', '乌克兰', '加入', '欧盟', '。'], [('欧盟', 'ORGANIZATION', 7, 8), ('北约', 'ORGANIZATION', 9, 10), ('乌克兰', 'LOCATION', 13, 14), ('北约', 'ORGANIZATION', 26, 27), ('欧盟', 'ORGANIZATION', 28, 29), ('乌克兰', 'LOCATION', 30, 31), ('多次', 'FREQUENCY', 33, 34), ('乌克兰', 'LOCATION', 41, 42), ('俄', 'LOCATION', 47, 48), ('乌', 'LOCATION', 48, 49), ('乌克兰', 'LOCATION', 54, 55), ('泽连斯基', 'PERSON', 56, 57), ('欧盟', 'ORGANIZATION', 58, 59), ('北约', 'ORGANIZATION', 60, 61), ('几日', 'DATE', 70, 71), ('泽连斯基', 'PERSON', 72, 73), ('欧盟', 'ORGANIZATION', 86, 87), ('乌克兰', 'LOCATION', 93, 94), ('欧盟', 'ORGANIZATION', 95, 96)])\n"
     ]
    }
   ],
   "source": [
    "tokenize = Tokenization()\n",
    "sents = tokenize.hanlp_token_ner(\"众所周知，长时间以来，加入欧盟和北约，就是乌克兰政府的心愿。过去这段时间里，为了加入北约和欧盟，乌克兰政府曾多次向其喊话，要求其同意乌克兰的加入。而自俄乌冲突正式开始后，乌克兰总统泽连斯基喊话欧盟与北约的频率更是越发频繁，就在前几日，泽连斯基更是发表了视频讲话，并在视频讲话中呼吁欧盟启动特殊程序，立即同意乌克兰加入欧盟。\")\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2f2a7-fc97-4d95-8dc4-6a09d7eb84ce",
   "metadata": {},
   "source": [
    "## 拼音转汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb00dc38-f154-43f6-9c69-8cf6697e9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ai hao/null]\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_pinyin_to_chinese():\n",
    "    \"\"\" HanLP中的数据结构和接口是灵活的，组合这些接口，可以自己创造新功能\n",
    "    >>> demo_pinyin_to_chinese()\n",
    "    [renmenrenweiyalujiangbujian/null, lvse/[滤色, 绿色]]\n",
    "    \"\"\"\n",
    "    AhoCorasickDoubleArrayTrie = JClass(\n",
    "        \"com.hankcs.hanlp.collection.AhoCorasick.AhoCorasickDoubleArrayTrie\")\n",
    "    StringDictionary = JClass(\n",
    "        \"com.hankcs.hanlp.corpus.dictionary.StringDictionary\")\n",
    "    CommonAhoCorasickDoubleArrayTrieSegment = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.CommonAhoCorasickDoubleArrayTrieSegment\")\n",
    "    CommonAhoCorasickSegmentUtil = JClass(\n",
    "        \"com.hankcs.hanlp.seg.Other.CommonAhoCorasickSegmentUtil\")\n",
    "    Config = JClass(\"com.hankcs.hanlp.HanLP$Config\")\n",
    "\n",
    "    TreeMap = JClass(\"java.util.TreeMap\")\n",
    "    TreeSet = JClass(\"java.util.TreeSet\")\n",
    "\n",
    "    dictionary = StringDictionary()\n",
    "    dictionary.load(Config.PinyinDictionaryPath)\n",
    "    entry = {}\n",
    "    m_map = TreeMap()\n",
    "    for entry in dictionary.entrySet():\n",
    "        pinyins = entry.getValue().replace(\"[\\\\d,]\", \"\")\n",
    "        words = m_map.get(pinyins)\n",
    "        if words is None:\n",
    "            words = TreeSet()\n",
    "            m_map.put(pinyins, words)\n",
    "        words.add(entry.getKey())\n",
    "    words = TreeSet()\n",
    "    words.add(\"绿色\")\n",
    "    words.add(\"滤色\")\n",
    "    m_map.put(\"lvse\", words)\n",
    "\n",
    "    segment = CommonAhoCorasickDoubleArrayTrieSegment(m_map)\n",
    "    print(segment.segment(\"ai hao\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_pinyin_to_chinese()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b344b57-1ca7-4d84-8efa-91c8402f1595",
   "metadata": {},
   "source": [
    "## 去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79bacad0-9021-4ed3-afbb-2988075dd99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stopwords'...\n",
      "remote: Enumerating objects: 22, done.\u001b[K\n",
      "remote: Total 22 (delta 0), reused 0 (delta 0), pack-reused 22\u001b[K\n",
      "Unpacking objects: 100% (22/22), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/goto456/stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c718058-55fc-40cf-a29c-f9b9c9388902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import jieba\n",
    "\n",
    "\n",
    "# 读取停用词列表\n",
    "def get_stopword_list(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:    # \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "# 分词 然后清除停用词语\n",
    "def clean_stopword(str, stopword_list):\n",
    "    result = ''\n",
    "    tokenize = Tokenization()\n",
    "    word_list = tokenize.hanlp_token_ner(str)[0]\n",
    "    for w in word_list:\n",
    "        if w not in stopword_list:\n",
    "            result += w\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44d88cb-1ce6-47bc-945f-970c81261b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'前日泽连斯基更发表视频讲话'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_file = 'stopwords/cn_stopwords.txt'\n",
    "stopword_list = get_stopword_list(stopword_file) \n",
    "clean_stopword(\"就在前几日，泽连斯基更是发表了视频讲话，\", stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b422092-ede1-4ea6-aaba-38666a758bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyhanlp\n",
      "  Downloading pyhanlp-0.1.84.tar.gz (136 kB)\n",
      "     |████████████████████████████████| 136 kB 942 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jpype1==0.7.0\n",
      "  Downloading JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "     |████████████████████████████████| 2.7 MB 6.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: hanlp-downloader in /usr/local/lib/python3.6/dist-packages (from pyhanlp) (0.0.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hanlp-downloader->pyhanlp) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (2.0.1)\n",
      "Building wheels for collected packages: pyhanlp\n",
      "  Building wheel for pyhanlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyhanlp: filename=pyhanlp-0.1.84-py3-none-any.whl size=29819 sha256=40b65c709827d38f2356b5cbd42b605849c693c05545735a1a8409f8bb99d3b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/45/6f/8e6f5fffdb1cb0b7c40d4e78fa41469102d7e324bf78fd7dc9\n",
      "Successfully built pyhanlp\n",
      "Installing collected packages: jpype1, pyhanlp\n",
      "Successfully installed jpype1-0.7.0 pyhanlp-0.1.84\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8b89edd-dca6-4088-93bc-e675e45bdeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    demo_custom_dictionary(text)\n",
      "Expecting:\n",
      "    [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "**********************************************************************\n",
      "File \"__main__\", line 10, in __main__.demo_custom_dictionary\n",
      "Failed example:\n",
      "    demo_custom_dictionary(text)\n",
      "Expected:\n",
      "    [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "Got:\n",
      "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "Trying:\n",
      "    demo_stopword()\n",
      "Expecting:\n",
      "    [小区/n, 反对/v, 喂养/v, 流浪猫/nz, 赞成/v, 喂养/v, 小宝贝/nz]\n",
      "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
      "    [小区/n, 居民/n, 有/vyou, 的/ude1, 反对/v, 喂养/v, 流浪猫/nz, ，/w, 而/cc, 有的/rz, 居民/n, 却/d, 赞成/v, 喂养/v, 这些/rz, 小宝贝/nz]\n",
      "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
      "    [数字/n, 123/m, 保留/v]\n",
      "**********************************************************************\n",
      "File \"__main__\", line 28, in __main__.demo_stopword\n",
      "Failed example:\n",
      "    demo_stopword()\n",
      "Exception raised:\n",
      "    Traceback (most recent call last):\n",
      "      File \"/usr/lib/python3.6/doctest.py\", line 1330, in __run\n",
      "        compileflags, 1), test.globs)\n",
      "      File \"<doctest __main__.demo_stopword[0]>\", line 1, in <module>\n",
      "        demo_stopword()\n",
      "      File \"<ipython-input-40-b9ffe828e723>\", line 55, in demo_stopword\n",
      "        MyFilter = JClass('MyFilter')\n",
      "      File \"/usr/local/lib/python3.6/dist-packages/jpype/_jclass.py\", line 130, in __new__\n",
      "        return _JClassNew(args[0], **kwargs)\n",
      "      File \"/usr/local/lib/python3.6/dist-packages/jpype/_jclass.py\", line 213, in _JClassNew\n",
      "        javaClass = _jpype.PyJPClass(arg)\n",
      "    jpype._jclass.java.lang.NoClassDefFoundError: MyFilter\n",
      "35 items had no tests:\n",
      "    __main__\n",
      "    __main__.QueryPre\n",
      "    __main__.QueryPre.__init__\n",
      "    __main__.QueryPre.capital_to_lower\n",
      "    __main__.QueryPre.filter_emoji\n",
      "    __main__.QueryPre.run\n",
      "    __main__.QueryPre.strB2Q\n",
      "    __main__.QueryPre.strQ2B\n",
      "    __main__.QueryPre.t2s_by_opencc\n",
      "    __main__.Tokenization\n",
      "    __main__.Tokenization.__init__\n",
      "    __main__.Tokenization.hanlp_token_ner\n",
      "    __main__.WordDiscovery\n",
      "    __main__.WordDiscovery.__init__\n",
      "    __main__.WordDiscovery.calculate_entropy\n",
      "    __main__.WordDiscovery.compute_aggregation\n",
      "    __main__.WordDiscovery.compute_entropys\n",
      "    __main__.WordDiscovery.compute_score\n",
      "    __main__.WordDiscovery.count_word\n",
      "    __main__.WordDiscovery.find_word\n",
      "    __main__._check_imported\n",
      "    __main__._jupyterlab_variableinspector_default\n",
      "    __main__._jupyterlab_variableinspector_deletevariable\n",
      "    __main__._jupyterlab_variableinspector_dict_list\n",
      "    __main__._jupyterlab_variableinspector_displaywidget\n",
      "    __main__._jupyterlab_variableinspector_getcontentof\n",
      "    __main__._jupyterlab_variableinspector_getmatrixcontent\n",
      "    __main__._jupyterlab_variableinspector_getshapeof\n",
      "    __main__._jupyterlab_variableinspector_getsizeof\n",
      "    __main__._jupyterlab_variableinspector_is_matrix\n",
      "    __main__._jupyterlab_variableinspector_is_widget\n",
      "    __main__.clean_stopword\n",
      "    __main__.get_stopword_list\n",
      "    __main__.load_dictionary\n",
      "    __main__.remove_stopwords\n",
      "**********************************************************************\n",
      "2 items had failures:\n",
      "   1 of   2 in __main__.demo_custom_dictionary\n",
      "   1 of   1 in __main__.demo_stopword\n",
      "3 tests in 37 items.\n",
      "1 passed and 2 failed.\n",
      "***Test Failed*** 2 failures.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pyhanlp.static import HANLP_JAR_PATH, STATIC_ROOT\n",
    "\n",
    "java_code_path = os.path.join(STATIC_ROOT, 'MyFilter.java')\n",
    "with open(java_code_path, 'w') as out:\n",
    "    java_code = \"\"\"\n",
    "import com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary;\n",
    "import com.hankcs.hanlp.dictionary.stopword.Filter;\n",
    "import com.hankcs.hanlp.seg.common.Term;\n",
    "public class MyFilter implements Filter\n",
    "{\n",
    "    public boolean shouldInclude(Term term)\n",
    "    {\n",
    "        if (term.nature.startsWith('m')) return true; // 数词保留\n",
    "        return !CoreStopWordDictionary.contains(term.word); // 停用词过滤\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "    out.write(java_code)\n",
    "os.system('javac -cp {} {} -d {}'.format(HANLP_JAR_PATH, java_code_path, STATIC_ROOT))\n",
    "# 编译结束才可以启动hanlp\n",
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_stopword():\n",
    "    \"\"\"\n",
    "    >>> demo_stopword()\n",
    "    [小区/n, 反对/v, 喂养/v, 流浪猫/nz, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 有/vyou, 的/ude1, 反对/v, 喂养/v, 流浪猫/nz, ，/w, 而/cc, 有的/rz, 居民/n, 却/d, 赞成/v, 喂养/v, 这些/rz, 小宝贝/nz]\n",
    "    [小区/n, 居民/n, 反对/v, 喂养/v, 流浪猫/nz, 居民/n, 赞成/v, 喂养/v, 小宝贝/nz]\n",
    "    [数字/n, 123/m, 保留/v]\n",
    "    \"\"\"\n",
    "    CoreStopWordDictionary = JClass(\"com.hankcs.hanlp.dictionary.stopword.CoreStopWordDictionary\")\n",
    "    Filter = JClass(\"com.hankcs.hanlp.dictionary.stopword.Filter\")\n",
    "    Term = JClass(\"com.hankcs.hanlp.seg.common.Term\")\n",
    "    BasicTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.BasicTokenizer\")\n",
    "    NotionalTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NotionalTokenizer\")\n",
    "\n",
    "    text = \"小区居民有的反对喂养流浪猫，而有的居民却赞成喂养这些小宝贝\"\n",
    "    # 可以动态修改停用词词典\n",
    "    CoreStopWordDictionary.add(\"居民\")\n",
    "    print(NotionalTokenizer.segment(text))\n",
    "    CoreStopWordDictionary.remove(\"居民\")\n",
    "    print(NotionalTokenizer.segment(text))\n",
    "\n",
    "    # 可以对任意分词器的结果执行过滤\n",
    "    term_list = BasicTokenizer.segment(text)\n",
    "    print(term_list)\n",
    "    CoreStopWordDictionary.apply(term_list)\n",
    "    print(term_list)\n",
    "\n",
    "    # 还可以自定义过滤逻辑\n",
    "    MyFilter = JClass('MyFilter')\n",
    "    CoreStopWordDictionary.FILTER = MyFilter()\n",
    "    print(NotionalTokenizer.segment(\"数字123的保留\"))  # “的”位于stopwords.txt所以被过滤，数字得到保留\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc9bb5-e6b9-4587-b708-5c0ad8a35f94",
   "metadata": {},
   "source": [
    "## 新词发现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2ec4cd8-d23f-49e7-b086-03ae2799ad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting macropodus\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/24/5c/95f458b3db3dee5afb5ef5344d880afa0985e0243f743340774c5844ffa9/Macropodus-0.0.7-py2.py3-none-any.whl (6.2 MB)\n",
      "     |████████████████████████████████| 6.2 MB 3.8 MB/s            \n",
      "\u001b[?25hCollecting passlib==1.7.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ee/a7/d6d238d927df355d4e4e000670342ca4705a72f0bf694027cf67d9bcf5af/passlib-1.7.1-py2.py3-none-any.whl (498 kB)\n",
      "     |████████████████████████████████| 498 kB 3.8 MB/s            \n",
      "\u001b[?25hCollecting networkx==2.4\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6 MB)\n",
      "     |████████████████████████████████| 1.6 MB 3.9 MB/s            \n",
      "\u001b[?25hCollecting gensim==3.7.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d7/b9/6c93685bed0026b6a1cce55ab173f6b617f6db0d1325d25489c2fd43e711/gensim-3.7.1-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "     |████████████████████████████████| 24.2 MB 2.8 MB/s            \n",
      "\u001b[?25hCollecting tqdm==4.31.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
      "     |████████████████████████████████| 48 kB 233 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from macropodus) (1.19.5)\n",
      "Collecting scikit-learn\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d3/eb/d0e658465c029feb7083139d9ead51000742e88b1fb7f1504e19e1b4ce6e/scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "     |████████████████████████████████| 22.2 MB 3.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from macropodus) (1.1.5)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.7.1->macropodus) (1.15.0)\n",
      "Collecting smart-open>=1.7.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/09/db/fab79b619923e26cecc5fb460c80f71f99666fe19182d5bb600ec4d6ff10/smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "     |████████████████████████████████| 58 kB 4.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.7.1->macropodus) (1.5.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.4->macropodus) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->macropodus) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->macropodus) (2021.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->macropodus) (1.1.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, smart-open, tqdm, scikit-learn, passlib, networkx, gensim, macropodus\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.5.1\n",
      "    Uninstalling networkx-2.5.1:\n",
      "      Successfully uninstalled networkx-2.5.1\n",
      "Successfully installed gensim-3.7.1 macropodus-0.0.7 networkx-2.4 passlib-1.7.1 scikit-learn-0.24.2 smart-open-6.0.0 threadpoolctl-3.1.0 tqdm-4.31.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple macropodus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c97cd8f-2b00-419a-ac8b-8482d911fbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 09:44:19,249 - seg_basic.py[line:19] - INFO: path of dict cache is /usr/local/lib/python3.6/dist-packages/macropodus/data/cache/macropodus.cache!\n",
      "2022-05-06 09:44:19,669 - textcleaner.py[line:37] - INFO: 'pattern' package not found; tag filters are not available for English\n",
      "2022-05-06 09:44:19,674 - word2vec.py[line:19] - INFO: path of w2v cache is /usr/local/lib/python3.6/dist-packages/macropodus/data/cache/word2vec_char.cache!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "页面 {'a': 2.361718, 'r': 2.281036, 'l': 2.446439, 'f': 10, 'ns': 1.655677, 's': 218.207762}\n",
      "网页 {'a': 1.993236, 'r': 2.584963, 'l': 1.918296, 'f': 6, 'ns': 1.425885, 's': 84.559919}\n",
      "链接 {'a': 3.200754, 'r': 1.584963, 'l': 2.321928, 'f': 5, 'ns': 1.398996, 's': 82.396116}\n",
      "投票 {'a': 3.569237, 'r': 1.0, 'l': 1.584963, 'f': 3, 'ns': 1.086026, 's': 18.431306}\n",
      "B页面 {'a': 3.378601, 'r': 1.0, 'l': 1.0, 'f': 2, 'ns': 0.949694, 's': 6.417272}\n",
      "计算 {'a': 3.361718, 'r': 1.0, 'l': 1.0, 'f': 2, 'ns': 0.947079, 's': 6.367628}\n",
      "质量 {'a': 3.361718, 'r': 1.0, 'l': 1.0, 'f': 2, 'ns': 0.947079, 's': 6.367628}\n",
      "等级 {'a': 1.534619, 'r': 1.0, 'l': 0.625815, 'f': 6, 'ns': 0.551465, 's': 3.17772}\n",
      "一个 {'a': 1.398538, 'r': 0.75, 'l': 0.5, 'f': 4, 'ns': 0.394292, 's': 0.827148}\n",
      "重要 {'a': 1.496618, 'r': 0.5, 'l': 0.792481, 'f': 3, 'ns': 0.415517, 's': 0.739231}\n",
      "来源 {'a': 1.47902, 'r': 0.5, 'l': 0.792481, 'f': 3, 'ns': 0.414049, 's': 0.727958}\n",
      "可以 {'a': 1.930859, 'r': 0.5, 'l': 0.5, 'f': 2, 'ns': 0.362936, 's': 0.350389}\n",
      "其他 {'a': 1.784619, 'r': 0.5, 'l': 0.5, 'f': 2, 'ns': 0.348103, 's': 0.310616}\n",
      "一种 {'a': 1.33278, 'r': 0.5, 'l': 0.5, 'f': 2, 'ns': 0.305015, 's': 0.203259}\n",
      "的来 {'a': 0.667039, 'r': 0.5, 'l': 0.5, 'f': 2, 'ns': 0.238863, 's': 0.079666}\n",
      "A页面 {'a': 0.3754, 'r': 0.528321, 'l': 0.528321, 'f': 3, 'ns': 0.141858, 's': 0.044593}\n",
      "页面的 {'a': 0.292144, 'r': 0.5, 'l': 0.666667, 'f': 4, 'ns': 0.085198, 's': 0.033187}\n",
      "的等级 {'a': 0.371847, 'r': 0.333333, 'l': 0.5, 'f': 4, 'ns': 0.02929, 's': 0.007261}\n",
      "成都 {'a': 1.930859, 'r': 0.5, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "Pa {'a': 3.069237, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "Pag {'a': 4.183614, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "Page {'a': 4.744058, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "PageR {'a': 5.228731, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "ag {'a': 2.700754, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "age {'a': 3.937959, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "ageR {'a': 4.744058, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "ageRa {'a': 5.028731, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "ge {'a': 2.832272, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "geR {'a': 4.271292, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "geRa {'a': 4.744058, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "geRan {'a': 5.228731, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "eR {'a': 3.200754, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "eRa {'a': 4.183614, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "eRan {'a': 4.9283, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "eRank {'a': 5.376124, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "Ra {'a': 3.069237, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "Ran {'a': 4.429269, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "Rank {'a': 5.112541, 'r': 1.584963, 'l': 0.0, 'f': 3, 'ns': 0.5046, 's': 0.0}\n",
      "an {'a': 3.069237, 'r': 0.0, 'l': 0.0, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "ank {'a': 4.429269, 'r': 1.584963, 'l': 0.0, 'f': 3, 'ns': 0.434461, 's': 0.0}\n",
      "nk {'a': 3.569237, 'r': 1.584963, 'l': 0.0, 'f': 3, 'ns': 0.357572, 's': 0.0}\n",
      "算法 {'a': 3.069237, 'r': 0.0, 'l': 1.0, 'f': 2, 'ns': 0.188224, 's': 0.0}\n",
      "互联 {'a': 3.861718, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "互联网 {'a': 4.378601, 'r': 1.0, 'l': 0.0, 'f': 2, 'ns': 0.310111, 's': 0.0}\n",
      "联网 {'a': 2.700754, 'r': 1.0, 'l': 0.0, 'f': 2, 'ns': 0.159222, 's': 0.0}\n",
      "的网 {'a': 0.538395, 'r': 0.0, 'l': 0.5, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "的网页 {'a': 0.255107, 'r': 0.333333, 'l': 0.333333, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "要的 {'a': 0.868877, 'r': 0.0, 'l': 0.5, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "要的信 {'a': 0.415178, 'r': 0.0, 'l': 0.333333, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "的信 {'a': 1.118877, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "思想 {'a': 3.861718, 'r': 0.0, 'l': 1.0, 'f': 2, 'ns': 0.258646, 's': 0.0}\n",
      "页面的等 {'a': 0.050977, 'r': 0.0, 'l': 0.25, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "页面的等级 {'a': 3.843531, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "面的 {'a': 0.788395, 'r': 0.75, 'l': 0.0, 'f': 4, 'ns': 0.0, 's': 0.0}\n",
      "面的等 {'a': 0.307515, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "面的等级 {'a': 3.616318, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "的等 {'a': 0.972637, 'r': 0.0, 'l': 0.75, 'f': 4, 'ns': 0.0, 's': 0.0}\n",
      "Go {'a': 3.361718, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "Goo {'a': 4.485911, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "Goog {'a': 4.970781, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "Googl {'a': 5.527102, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "oo {'a': 2.861718, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "oog {'a': 4.045268, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "oogl {'a': 4.970781, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "oogle {'a': 5.262716, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "og {'a': 2.700754, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "ogl {'a': 4.378601, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "ogle {'a': 4.890299, 'r': 1.0, 'l': 0.0, 'f': 2, 'ns': 0.364679, 's': 0.0}\n",
      "gl {'a': 3.200754, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "gle {'a': 4.271292, 'r': 1.0, 'l': 0.0, 'f': 2, 'ns': 0.299102, 's': 0.0}\n",
      "le {'a': 3.200754, 'r': 1.0, 'l': 0.0, 'f': 2, 'ns': 0.199152, 's': 0.0}\n",
      "A页 {'a': 1.180859, 'r': 0.0, 'l': 0.792481, 'f': 3, 'ns': 0.0, 's': 0.0}\n",
      "B页 {'a': 2.361718, 'r': 0.0, 'l': 1.0, 'f': 2, 'ns': 0.134532, 's': 0.0}\n",
      "的页 {'a': 0.368877, 'r': 0.0, 'l': 0.5, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "的页面 {'a': 0.255107, 'r': 0.0, 'l': 0.333333, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "就是 {'a': 1.22902, 'r': 0.0, 'l': 0.5, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "假设 {'a': 3.569237, 'r': 0.0, 'l': 0.918296, 'f': 3, 'ns': 0.209588, 's': 0.0}\n",
      "量假 {'a': 3.069237, 'r': 0.0, 'l': 1.0, 'f': 2, 'ns': 0.188224, 's': 0.0}\n",
      "量假设 {'a': 4.429269, 'r': 0.0, 'l': 1.0, 'f': 2, 'ns': 0.315364, 's': 0.0}\n",
      "一个网 {'a': 0.370612, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "一个网页 {'a': 0.057632, 'r': 0.25, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "个网 {'a': 1.019895, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "个网页 {'a': 0.32644, 'r': 0.333333, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "就越 {'a': 1.430859, 'r': 0.0, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "就越重 {'a': 0.449474, 'r': 0.333333, 'l': 0.0, 'f': 2, 'ns': 0.0, 's': 0.0}\n",
      "越重 {'a': 2.700754, 'r': 1.0, 'l': 0.0, 'f': 2, 'ns': 0.159222, 's': 0.0}\n",
      "\n",
      "#################\n",
      "\n",
      "请输入:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 一个很好的故事\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-803e41a86d1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"请输入:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         res = wd.find_word(text=ques, use_type=\"text\", use_avg=False, use_filter=False, use_output=True,\n\u001b[1;32m    264\u001b[0m                            freq_min=2, len_max=5, entropy_min=2.0, aggregation_min=3.2)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from collections import Counter, OrderedDict\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "from macropodus.data.words_common.stop_words import stop_words\n",
    "from macropodus.preprocess.tools_ml import cut_sentence, get_ngrams\n",
    "\n",
    "\n",
    "class WordDiscovery:\n",
    "    def __init__(self):\n",
    "        from macropodus.segment import segs\n",
    "        self.dict_words_freq = segs.dict_words_freq\n",
    "        self.algorithm = \"new-word-discovery\"\n",
    "        self.stop_words = stop_words\n",
    "        self.total_words_len = {}\n",
    "        self.total_words = 0\n",
    "        self.freq_min = 3\n",
    "        self.len_max = 7\n",
    "        self.round = 6\n",
    "        self.eps = 1e-9\n",
    "        self.empty_words = [sw for sw in stop_words.values() if len(sw)==1] # 虚词\n",
    "\n",
    "    def count_word(self, text, use_type=\"text\"):\n",
    "        \"\"\"\n",
    "            词频统计(句子/段落/文章)\n",
    "        :param text: str, path or doc, like \"大漠帝国。\" or \"/home/data/doc.txt\"\n",
    "        :param use_type: str,  \"text\" or \"file\", file of \"utf-8\" of \"txt\"\n",
    "        :return: class<Counter>, word-freq\n",
    "        \"\"\"\n",
    "        import macropodus\n",
    "        self.words_count = Counter()\n",
    "        if use_type==\"text\": # 输入为文本形式\n",
    "            text = macropodus.han2zh(text)\n",
    "            texts = cut_sentence(use_type=self.algorithm,\n",
    "                                 text=text)  # 切句子, 如中英文的逗号/句号/感叹号\n",
    "            for text in texts:\n",
    "                n_grams = get_ngrams(use_type=self.algorithm,\n",
    "                                     len_max=self.len_max,\n",
    "                                     text=text) # 获取一个句子的所有n-gram\n",
    "                self.words_count.update(n_grams)\n",
    "        elif use_type==\"file\": # 输入为文件形式\n",
    "            if not os.path.exists(text):\n",
    "                raise RuntimeError(\"path of text must exist!\")\n",
    "            fr8 = open(text, \"r\", encoding=\"utf-8\")\n",
    "            for text in fr8:\n",
    "                if text.strip():\n",
    "                    text = macropodus.han2zh(text)\n",
    "                    texts = cut_sentence(use_type=self.algorithm,\n",
    "                                         text=text) # 切句子, 如中英文的逗号/句号/感叹号\n",
    "                    for text in texts:\n",
    "                        n_grams = get_ngrams(use_type=self.algorithm,\n",
    "                                             len_max=self.len_max,\n",
    "                                             text=text)  # 获取一个句子的所有n-gram\n",
    "                        self.words_count.update(n_grams)\n",
    "            fr8.close()\n",
    "        else:\n",
    "            raise RuntimeError(\"use_type must be 'text' or 'file'\")\n",
    "        self.total_words = sum(self.words_count.values())\n",
    "\n",
    "    def calculate_entropy(self, boundary_type=\"left\"):\n",
    "        \"\"\"\n",
    "            计算左熵和右熵\n",
    "        :param boundary_type: str, like \"left\" or \"right\"\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # 获取成词的最左边和最右边的一个字\n",
    "        one_collect = {}\n",
    "        self.total_words_len = {}\n",
    "        for k, v in self.words_count.items():\n",
    "            len_k = len(k)\n",
    "            if len_k >= 2:  # 词长度大于3\n",
    "                if boundary_type == \"right\":\n",
    "                    k_boundary = k[:-1]\n",
    "                else:\n",
    "                    k_boundary = k[1:]\n",
    "                # 左右边, 保存为dict, 左右丰度\n",
    "                if k_boundary in self.words_count:\n",
    "                    if k_boundary not in one_collect:\n",
    "                        one_collect[k_boundary] = [v]\n",
    "                    else:\n",
    "                        one_collect[k_boundary] = one_collect[k_boundary] + [v]\n",
    "            # 计算n-gram的长度\n",
    "            if len_k not in self.total_words_len:\n",
    "                self.total_words_len[len_k] = [v]\n",
    "            else:\n",
    "                self.total_words_len[len_k] += [v]\n",
    "        self.total_words_len = dict([(k, sum(v)) for k,v in self.total_words_len.items()])\n",
    "\n",
    "        # 计算左右熵\n",
    "        for k, v in self.words_select.items():\n",
    "            # 从字典获取\n",
    "            boundary_v = one_collect.get(k, None)\n",
    "            # 计算候选词的左右凝固度, 取最小的那个\n",
    "            if boundary_v:\n",
    "                # 求和\n",
    "                sum_boundary = sum(boundary_v)\n",
    "                # 计算信息熵\n",
    "                entroy_boundary = sum([-(enum_bo / sum_boundary) * math.log(enum_bo / sum_boundary, 2)\n",
    "                                       for enum_bo in boundary_v])\n",
    "            else:\n",
    "                entroy_boundary = 0.0\n",
    "            # 惩罚虚词开头或者结尾\n",
    "            if (k[0] in self.empty_words or k[-1] in self.empty_words):\n",
    "                entroy_boundary = entroy_boundary / len(k)\n",
    "            if boundary_type == \"right\":\n",
    "                self.right_entropy[k] = round(entroy_boundary, self.round)\n",
    "            else:\n",
    "                self.left_entropy[k] = round(entroy_boundary, self.round)\n",
    "\n",
    "    def compute_entropys(self):\n",
    "        \"\"\"\n",
    "            计算左右熵\n",
    "        :param words_count:dict, like {\"我\":32, \"你们\":12} \n",
    "        :param len_max: int, like 6\n",
    "        :param freq_min: int, like 32\n",
    "        :return: dict\n",
    "        \"\"\"\n",
    "        # 提取大于最大频率的词语, 以及长度在3-len_max的词语\n",
    "        self.words_select = {word: count for word, count in self.words_count.items()\n",
    "                             if count >= self.freq_min and \" \" not in word\n",
    "                             and 1 < len(word) <= self.len_max\n",
    "                             }\n",
    "        # 计算凝固度, 左右两边\n",
    "        self.right_entropy = {}\n",
    "        self.left_entropy = {}\n",
    "        self.calculate_entropy(boundary_type=\"left\")\n",
    "        self.calculate_entropy(boundary_type=\"right\")\n",
    "\n",
    "    def compute_aggregation(self):\n",
    "        \"\"\"\n",
    "            计算凝固度\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        twl_1 = self.total_words_len[1] # ngram=1 的所有词频\n",
    "        self.aggregation = {}\n",
    "        for word, value in self.words_select.items():\n",
    "            len_word = len(word)\n",
    "            twl_n = self.total_words_len[len_word] # ngram=n 的所有词频\n",
    "            words_freq = [self.words_count.get(wd, 1) for wd in word]\n",
    "            probability_word = value / twl_n\n",
    "            probability_chars = reduce(mul,([wf for wf in words_freq])) / (twl_1**(len(word)))\n",
    "            pmi = math.log(probability_word / probability_chars, 2)\n",
    "            # AMI=PMI/length_word. 惩罚虚词(避免\"的\", \"得\", \"了\"开头结尾的情况)\n",
    "            word_aggregation = pmi/(len_word**len_word) if (word[0] in self.empty_words or word[-1] in self.empty_words) \\\n",
    "                                                        else pmi/len_word # pmi / len_word / len_word\n",
    "            self.aggregation[word] = round(word_aggregation, self.round)\n",
    "\n",
    "    def compute_score(self, word, value, a, r, l, rl, lambda_0, lambda_3):\n",
    "        \"\"\"\n",
    "            计算最终得分\n",
    "        :param word: str, word with prepare\n",
    "        :param value: float, word freq\n",
    "        :param a: float, aggregation of word\n",
    "        :param r: float, right entropy of word\n",
    "        :param l: float, left entropy of word\n",
    "        :param rl: float, right_entropy * left_entropy\n",
    "        :param lambda_0: lambda 0\n",
    "        :param lambda_3: lambda 3\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.new_words[word] = {}\n",
    "        # math.log10(self.aggregation[word]) - math.log10(self.total_words)\n",
    "        self.new_words[word][\"a\"] = a\n",
    "        self.new_words[word][\"r\"] = r\n",
    "        self.new_words[word][\"l\"] = l\n",
    "        self.new_words[word][\"f\"] = value\n",
    "        # word-liberalization\n",
    "        m1 = lambda_0(r)\n",
    "        m2 = lambda_0(l)\n",
    "        m3 = lambda_0(a)\n",
    "        score_ns = lambda_0((lambda_3(m1, m2) + lambda_3(m1, m3) + lambda_3(m2, m3)) / 3)\n",
    "        self.new_words[word][\"ns\"] = round(score_ns, self.round)\n",
    "        # 乘以词频word-freq, 连乘是为了防止出现较小项\n",
    "        score_s = value * a * rl * score_ns\n",
    "        self.new_words[word][\"s\"] = round(score_s, self.round)\n",
    "\n",
    "    def find_word(self, text, use_type=\"text\", freq_min=2, len_max=5, entropy_min=2.0, aggregation_min=3.2,\n",
    "                        use_output=True, use_avg=False, use_filter=False):\n",
    "        \"\"\"\n",
    "            新词发现与策略\n",
    "        :param text: str, path or doc, like \"大漠帝国。\" or \"/home/data/doc.txt\"\n",
    "        :param use_type: str,  输入格式, 即文件输入还是文本输入, \"text\" or \"file\", file of \"utf-8\" of \"txt\"\n",
    "        :param use_output: bool,  输出模式, 即最后结果是否全部输出\n",
    "        :param use_filter: bool,  新词过滤, 即是否过滤macropodus词典和停用词\n",
    "        :param freq_min: int, 最小词频, 大于1\n",
    "        :param len_max: int, 最大成词长度, 一般为5, 6, 7\n",
    "        :param entropy_min: int, 左右熵阈值, 低于则过滤\n",
    "        :param aggregation_min: int, PMI(凝固度)-阈值, 低于则过滤\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        self.aggregation_min = aggregation_min\n",
    "        self.entropy_min = entropy_min\n",
    "        self.freq_min = freq_min\n",
    "        self.len_max = len_max\n",
    "        self.count_word(text=text, use_type=use_type)\n",
    "        self.compute_entropys()\n",
    "        self.compute_aggregation()\n",
    "        self.new_words = {}\n",
    "        lambda_3 = lambda m1, m2: math.log((m1 * math.e ** m2 + m2 * math.e ** m1 + self.eps) / (abs(m1 - m2) + 1), 10)\n",
    "        lambda_0 = lambda x: -self.eps * x + self.eps if x <= 0 else x\n",
    "        # 输出\n",
    "        for word, value in self.words_select.items():\n",
    "            # 过滤通用词\n",
    "            if use_filter and word in self.dict_words_freq:\n",
    "                continue\n",
    "            # 过滤停用词\n",
    "            if word in self.stop_words:\n",
    "                continue\n",
    "            # {\"aggregation\":\"a\", \"right_entropy\":\"r\", \"left_entropy\":\"l\", \"frequency\":\"f\",\n",
    "            #  \"word-liberalization\":\"ns\", \"score\":\"s\"}\n",
    "            a = self.aggregation[word]\n",
    "            r = self.right_entropy[word]\n",
    "            l = self.left_entropy[word]\n",
    "            rl = (r+l) / 2 if use_avg else r * l\n",
    "            if use_output or (use_avg and a > self.aggregation_min and rl > self.entropy_min) or \\\n",
    "                             (not use_avg and a > self.aggregation_min and r > self.entropy_min and l > self.entropy_min):\n",
    "                self.compute_score(word, value, a, r, l, rl, lambda_0, lambda_3)\n",
    "\n",
    "        # 排序\n",
    "        self.new_words = sorted(self.new_words.items(), key=lambda x:x[1][\"s\"], reverse=True)\n",
    "        self.new_words = OrderedDict(self.new_words)\n",
    "        return self.new_words\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from macropodus.preprocess.tools_common import (\n",
    "        load_json,\n",
    "        save_json,\n",
    "        txt_read,\n",
    "        txt_write,\n",
    "    )\n",
    "\n",
    "    summary = \"四川发文取缔全部不合规p2p。字节跳动与今日头条。成都日报，成都市，李太白与杜甫\" \\\n",
    "              \"PageRank算法简介。\" \\\n",
    "              \"是上世纪90年代末提出的一种计算网页权重的算法! \" \\\n",
    "              \"当时，互联网技术突飞猛进，各种网页网站爆炸式增长。 \" \\\n",
    "              \"业界急需一种相对比较准确的网页重要性计算方法。 \" \\\n",
    "              \"是人们能够从海量互联网世界中找出自己需要的信息。 \" \\\n",
    "              \"百度百科如是介绍他的思想:PageRank通过网络浩瀚的超链接关系来确定一个页面的等级。 \" \\\n",
    "              \"Google把从A页面到B页面的链接解释为A页面给B页面投票。 \" \\\n",
    "              \"Google根据投票来源甚至来源的来源，即链接到A页面的页面。 \" \\\n",
    "              \"和投票目标的等级来决定新的等级。简单的说， \" \\\n",
    "              \"一个高等级的页面可以使其他低等级页面的等级提升。 \" \\\n",
    "              \"具体说来就是，PageRank有两个基本思想，也可以说是假设。 \" \\\n",
    "              \"即数量假设：一个网页被越多的其他页面链接，就越重）。 \" \\\n",
    "              \"质量假设：一个网页越是被高质量的网页链接，就越重要。 \" \\\n",
    "              \"总的来说就是一句话，从全局角度考虑，获取重要的信。 \"\n",
    "\n",
    "    # 新词发现-文本\n",
    "    wd = WordDiscovery()\n",
    "    res = wd.find_word(text=summary, use_type=\"text\", use_avg=False, use_filter=False, use_output=True,\n",
    "                       freq_min=2, len_max=5, entropy_min=2.0, aggregation_min=3.2)\n",
    "    for k, v in res.items():\n",
    "        print(k, v)\n",
    "    print(\"\\n#################\\n\")\n",
    "\n",
    "    while True:\n",
    "        print(\"请输入:\")\n",
    "        ques = input()\n",
    "        res = wd.find_word(text=ques, use_type=\"text\", use_avg=False, use_filter=False, use_output=True,\n",
    "                           freq_min=2, len_max=5, entropy_min=2.0, aggregation_min=3.2)\n",
    "        for k, v in res.items():\n",
    "            print(k, v)\n",
    "    # ms = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae6bfb-76af-4ea6-b2e7-0d712a2f3732",
   "metadata": {},
   "source": [
    "## 领域词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74307e81-2cd1-45d4-9dd8-172a3fbaa977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyhanlp\n",
      "  Downloading pyhanlp-0.1.84.tar.gz (136 kB)\n",
      "     |████████████████████████████████| 136 kB 942 kB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jpype1==0.7.0\n",
      "  Downloading JPype1-0.7.0-cp36-cp36m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "     |████████████████████████████████| 2.7 MB 6.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: hanlp-downloader in /usr/local/lib/python3.6/dist-packages (from pyhanlp) (0.0.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hanlp-downloader->pyhanlp) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests->hanlp-downloader->pyhanlp) (2.0.1)\n",
      "Building wheels for collected packages: pyhanlp\n",
      "  Building wheel for pyhanlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyhanlp: filename=pyhanlp-0.1.84-py3-none-any.whl size=29819 sha256=40b65c709827d38f2356b5cbd42b605849c693c05545735a1a8409f8bb99d3b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/c5/45/6f/8e6f5fffdb1cb0b7c40d4e78fa41469102d7e324bf78fd7dc9\n",
      "Successfully built pyhanlp\n",
      "Installing collected packages: jpype1, pyhanlp\n",
      "Successfully installed jpype1-0.7.0 pyhanlp-0.1.84\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyhanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9702800-3b03-4f61-a5d6-ab02d19bcdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    demo_custom_dictionary(text)\n",
      "Expecting:\n",
      "    [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
      "ok\n",
      "35 items had no tests:\n",
      "    __main__\n",
      "    __main__.QueryPre\n",
      "    __main__.QueryPre.__init__\n",
      "    __main__.QueryPre.capital_to_lower\n",
      "    __main__.QueryPre.filter_emoji\n",
      "    __main__.QueryPre.run\n",
      "    __main__.QueryPre.strB2Q\n",
      "    __main__.QueryPre.strQ2B\n",
      "    __main__.QueryPre.t2s_by_opencc\n",
      "    __main__.Tokenization\n",
      "    __main__.Tokenization.__init__\n",
      "    __main__.Tokenization.hanlp_token_ner\n",
      "    __main__.WordDiscovery\n",
      "    __main__.WordDiscovery.__init__\n",
      "    __main__.WordDiscovery.calculate_entropy\n",
      "    __main__.WordDiscovery.compute_aggregation\n",
      "    __main__.WordDiscovery.compute_entropys\n",
      "    __main__.WordDiscovery.compute_score\n",
      "    __main__.WordDiscovery.count_word\n",
      "    __main__.WordDiscovery.find_word\n",
      "    __main__._check_imported\n",
      "    __main__._jupyterlab_variableinspector_default\n",
      "    __main__._jupyterlab_variableinspector_deletevariable\n",
      "    __main__._jupyterlab_variableinspector_dict_list\n",
      "    __main__._jupyterlab_variableinspector_displaywidget\n",
      "    __main__._jupyterlab_variableinspector_getcontentof\n",
      "    __main__._jupyterlab_variableinspector_getmatrixcontent\n",
      "    __main__._jupyterlab_variableinspector_getshapeof\n",
      "    __main__._jupyterlab_variableinspector_getsizeof\n",
      "    __main__._jupyterlab_variableinspector_is_matrix\n",
      "    __main__._jupyterlab_variableinspector_is_widget\n",
      "    __main__.clean_stopword\n",
      "    __main__.get_stopword_list\n",
      "    __main__.load_dictionary\n",
      "    __main__.remove_stopwords\n",
      "1 items passed all tests:\n",
      "   2 tests in __main__.demo_custom_dictionary\n",
      "2 tests in 36 items.\n",
      "2 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "from pyhanlp import *\n",
    "\n",
    "\n",
    "def demo_custom_dictionary(text):\n",
    "    \"\"\" 演示用户词典的动态增删\n",
    "    TO-DO:\n",
    "    DoubleArrayTrie分词\n",
    "    首字哈希之后二分的trie树分词\n",
    "    >>> text = \"攻城狮逆袭单身狗，迎娶白富美，走上人生巅峰\"  # 怎么可能噗哈哈！\n",
    "    >>> demo_custom_dictionary(text)\n",
    "    [攻城/vi, 狮/ng, 逆袭/nz, 单身/n, 狗/n, ，/w, 迎娶/v, 白富美/nr, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
    "    [攻城狮/nz, 逆袭/nz, 单身狗/nz, ，/w, 迎娶/v, 白富美/nz, ，/w, 走上/v, 人生/n, 巅峰/n]\n",
    "    \"\"\"\n",
    "    print(HanLP.segment(text))\n",
    "\n",
    "    CustomDictionary = JClass(\"com.hankcs.hanlp.dictionary.CustomDictionary\")\n",
    "    CustomDictionary.add(\"攻城狮\")  # 动态增加\n",
    "    CustomDictionary.insert(\"白富美\", \"nz 1024\")  # 强行插入\n",
    "    #CustomDictionary.remove(\"攻城狮\"); # 删除词语（注释掉试试）\n",
    "    CustomDictionary.add(\"单身狗\", \"nz 1024 n 1\")\n",
    "    #print(CustomDictionary.get(\"单身狗\"))\n",
    "\n",
    "    print(HanLP.segment(text))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991b3c0-ff60-4695-bcb8-7b2c903e1384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
